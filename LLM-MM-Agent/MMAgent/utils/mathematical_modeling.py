from agent.retrieve_method import MethodRetriever
from agent.task_solving import TaskSolver
from prompt.template import TASK_ANALYSIS_APPEND_PROMPT, TASK_FORMULAS_APPEND_PROMPT, TASK_MODELING_APPEND_PROMPT


def get_dependency_prompt(with_code, coordinator, task_id):
    task_dependency = [int(i) for i in coordinator.DAG[str(task_id)]]
    dependent_file_prompt = ""
    if len(task_dependency) > 0:
        # P2-8 FIX: Defensive bounds checking for task_dependency_analysis array
        # Prevents IndexError when task_id - 1 is out of bounds
        dependency_analysis_text = ""
        if hasattr(coordinator, 'task_dependency_analysis'):
            if coordinator.task_dependency_analysis is not None:
                # Check if task_id - 1 is within valid range
                if 0 <= (task_id - 1) < len(coordinator.task_dependency_analysis):
                    dependency_analysis_text = coordinator.task_dependency_analysis[task_id - 1]
                else:
                    # Index out of bounds - use generic message
                    dependency_analysis_text = f"Tasks {task_dependency} (dependency analysis not available for task {task_id})"
            else:
                dependency_analysis_text = f"Tasks {task_dependency} (no dependency analysis performed)"
        else:
            dependency_analysis_text = f"Tasks {task_dependency} (dependency tracking not initialized)"

        dependency_prompt = f"""\
This task is Task {task_id}, which depends on the following tasks: {task_dependency}. The dependencies for this task are analyzed as follows: {dependency_analysis_text}
"""
        for id in task_dependency:
            dependency_prompt += f"""\
---
# The Description of Task {id}:
{coordinator.memory[str(id)]['task_description']}
# The modeling method for Task {id}:
{coordinator.memory[str(id)]['mathematical_modeling_process']}
"""
            if with_code:
                dependency_prompt += f"""\
# The structure of code for Task {id}:
{coordinator.code_memory[str(id)]}
# The result for Task {id}:
{coordinator.memory[str(id)]['solution_interpretation']}
---
"""
                dependent_file_prompt += f"""\
# The files generated by code for Task {id}:
{coordinator.code_memory[str(id)]}
"""
                # Safer: explicitly include file outputs if present
                file_outputs = coordinator.code_memory.get(str(id), {}).get('file_outputs', [])
                if file_outputs:
                    dependent_file_prompt += f"\n# File outputs list for Task {id}:\n{file_outputs}\n"
            else:
                dependency_prompt += f"""\
# The result for Task {id}:
{coordinator.memory[str(id)]['solution_interpretation']}
---
"""
                
    if len(task_dependency) > 0:
        task_analysis_prompt = dependency_prompt + TASK_ANALYSIS_APPEND_PROMPT
        task_formulas_prompt = dependency_prompt + TASK_FORMULAS_APPEND_PROMPT
        task_modeling_prompt = dependency_prompt + TASK_MODELING_APPEND_PROMPT
    else:
        task_analysis_prompt = ""
        task_formulas_prompt = ""
        task_modeling_prompt = ""
    return task_analysis_prompt, task_formulas_prompt, task_modeling_prompt, dependent_file_prompt


def mathematical_modeling(task_id, problem, task_descriptions, llm, config, coordinator, with_code, logger_manager=None):
    """
    Perform mathematical modeling for a specific task.

    Args:
        task_id: Task identifier
        problem: Problem dictionary
        task_descriptions: List of task descriptions
        llm: Language model instance
        config: Configuration dictionary
        coordinator: Coordinator instance for managing task dependencies
        with_code: Whether code execution is involved
        logger_manager: Optional MMExperimentLogger for structured logging

    Returns:
        Tuple of (task_description, task_analysis, task_modeling_formulas, task_modeling_method, dependent_file_prompt)
    """
    # Log modeling start
    if logger_manager:
        logger_manager.log_progress(f"Task {task_id}: Mathematical Modeling", level='info')

    # CRITICAL FIX: Pass logger_manager to TaskSolver for proper error logging to errors.log
    ts = TaskSolver(llm, logger_manager)
    mr = MethodRetriever(llm)
    task_analysis_prompt, task_formulas_prompt, task_modeling_prompt, dependent_file_prompt = get_dependency_prompt(with_code, coordinator, task_id)

    # Task analysis - with bounds checking
    if 1 <= task_id <= len(task_descriptions):
        task_description = task_descriptions[task_id - 1]
    else:
        # Fallback: generate generic task description if task_id is out of range
        if logger_manager:
            logger_manager.log_progress(f"[WARNING] Task {task_id} ID out of range (1-{len(task_descriptions)}), using generic description", level='warning')
        task_description = f"Task {task_id}: Mathematical modeling and computational analysis"

    task_analysis = ts.analysis(task_analysis_prompt, task_description)
    
    # Hierarchical Modeling Knowledge Retrieval
    description_and_analysis = f'## Task Description\n{task_description}\n\n## Task Analysis\n{task_analysis}'
    top_modeling_methods = mr.retrieve_methods(description_and_analysis, top_k=config['top_method_num'])

    # Task Modeling
    task_modeling_formulas, task_modeling_method = ts.modeling(task_formulas_prompt, task_modeling_prompt, problem['data_description'], task_description, task_analysis, top_modeling_methods, round=config['task_formulas_round'])
    
    return task_description, task_analysis, task_modeling_formulas, task_modeling_method, dependent_file_prompt

    